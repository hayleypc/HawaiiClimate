{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCiy0vZgPwm5wqlwvAn5jQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hayleypc/HawaiiClimate/blob/main/soil_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FJSR_lUJAuGW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfASAHA7B1a0",
        "outputId": "76ed6447-5a3c-4e30-b79d-f6156e2c2d22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd"
      ],
      "metadata": {
        "id": "FUiG_4XBCkTW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator model\n",
        "def build_generator(latent_dim, sequence_length):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(256, activation='relu', input_dim=latent_dim),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dense(sequence_length, activation='tanh')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "uUacgusjBMnm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the discriminator model\n",
        "def build_discriminator(sequence_length):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(1024, activation='relu', input_dim=sequence_length),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "hywSkpYkBOk-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = models.Sequential([\n",
        "        generator,\n",
        "        discriminator\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "xuMwXpo5BRMu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the training function to include validation\n",
        "def train_gan(generator, discriminator, gan, epochs, batch_size, latent_dim, val_gen,keep_cols = [0,1,2,3]):\n",
        "    for epoch in range(epochs):\n",
        "        # Train on batches from the training generator\n",
        "        for _ in range(len(train_sequences) // batch_size):\n",
        "            # Generate fake sequences\n",
        "            noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "            fake_sequences = generator.predict(noise)\n",
        "\n",
        "            # Get a batch of real sequences from the training generator\n",
        "            real_sequences = next(train_gen)\n",
        "\n",
        "            # Labels for real and fake data\n",
        "            real_labels = tf.ones((batch_size, 1))\n",
        "            fake_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "            # Train the discriminator\n",
        "            discriminator.trainable = True\n",
        "            d_loss_real = discriminator.train_on_batch(real_sequences, real_labels)\n",
        "            d_loss_fake = discriminator.train_on_batch(fake_sequences, fake_labels)\n",
        "            d_loss = 0.5 * tf.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train the generator\n",
        "            discriminator.trainable = False\n",
        "            noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "            g_loss = gan.train_on_batch(noise, real_labels)\n",
        "\n",
        "        # Validation\n",
        "        val_real_sequences = next(val_gen)\n",
        "        val_fake_sequences = generator.predict(tf.random.normal(shape=(batch_size, latent_dim)))\n",
        "        val_d_loss_real = discriminator.evaluate(val_real_sequences, tf.ones((batch_size, 1)), verbose=0)\n",
        "        val_d_loss_fake = discriminator.evaluate(val_fake_sequences, tf.zeros((batch_size, 1)), verbose=0)\n",
        "        val_d_loss = 0.5 * np.add(val_d_loss_real, val_d_loss_fake)\n",
        "\n",
        "        # Print the progress\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}, D Loss: {d_loss}, G Loss: {g_loss}, Val D Loss: {val_d_loss}\")\n",
        "\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "          # data = validate_generator()\n",
        "          data = validate_generator(keep_cols = keep_cols)\n",
        "          x_values = [ datum[0] for datum in data]\n",
        "          y_values = [ datum[1] for datum in data]\n",
        "\n",
        "          # Create a scatter plot\n",
        "          plt.scatter(x_values, y_values, color='blue')\n",
        "\n",
        "          # Add labels and title\n",
        "          plt.xlabel('Sample Index')\n",
        "          plt.ylabel('Difference between Real and Generated')\n",
        "          plt.title('Scatter Plot of Validation Differences')\n",
        "\n",
        "          # Show the plot\n",
        "          plt.show()\n"
      ],
      "metadata": {
        "id": "b6hLGfIgBVGZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmnE9Ykxqf3H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_generator(n_lowest = 10, n_samp = 256,keep_cols = [0,1,2,3]):\n",
        "  # n_lowest = 10\n",
        "  result_vec = []\n",
        "\n",
        "  batch_val = next(val_gen)\n",
        "  for batch_ in batch_val:\n",
        "    noise = tf.random.normal(shape=(n_samp, latent_dim))\n",
        "    synthetic_samples = generator.predict(noise)\n",
        "    # batch_ = batch_val[1]\n",
        "    match_indices = [i for i in range(len(keep_cols)-1)]\n",
        "    # keep_cols[:(len(keep_cols)-1)]\n",
        "\n",
        "    data = [np.mean(np.sqrt((batch_[match_indices] - samp[match_indices])**2)) for samp in synthetic_samples]\n",
        "\n",
        "    indices = sorted(range(len(data)), key=lambda i: data[i])[:n_lowest]\n",
        "\n",
        "    lowest_data = [data[i] for i in indices]\n",
        "\n",
        "    most_similar = [synthetic_samples[i] for i in indices]\n",
        "\n",
        "    most_similar_scaled_df = pd.DataFrame(scaler.inverse_transform(pd.DataFrame(most_similar, columns=numeric_cols.columns)), columns=numeric_cols.columns)\n",
        "\n",
        "    original_scaled_df = pd.DataFrame(scaler.inverse_transform(pd.DataFrame(batch_.reshape(1,-1), columns=numeric_cols.columns)), columns=numeric_cols.columns)\n",
        "\n",
        "    error_vec = [np.mean(original_scaled_df[\"imp_c_float\"]) ,np.mean(most_similar_scaled_df[\"imp_c_float\"]) ]\n",
        "\n",
        "    # # most_similar_scaled_df = scaler.inverse_transform( most_similar_df)\n",
        "\n",
        "    # original_scaled = [scaler.inverse_transform(sim.reshape(1,-1)) for sim in [batch_]]\n",
        "\n",
        "    # most_similar_scaled_2d = [np.squeeze(sim) for sim in most_similar_scaled]\n",
        "\n",
        "    # original_scaled_2d = [np.squeeze(sim) for sim in original_scaled]\n",
        "\n",
        "    # most_similar_scaled_df = pd.DataFrame(most_similar_scaled_2d, columns=numeric_cols.columns)\n",
        "\n",
        "    # original_scaled_df = pd.DataFrame(original_scaled_2d, columns=numeric_cols.columns)\n",
        "\n",
        "    # result = [np.mean(most_similar_scaled_df['imp_c_float']), np.mean(original_scaled_df['imp_c_float'])]\n",
        "    # print(np.mean(original_scaled_df['agbd_m']))\n",
        "    result_vec.append(error_vec)\n",
        "  return(result_vec)"
      ],
      "metadata": {
        "id": "P7L77v0fvrTh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/sequence_data.csv'"
      ],
      "metadata": {
        "id": "46NsUEt2B80A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_gpd = gpd.read_file('/content/drive/MyDrive/hawaii_soils/Analysis Data/250_summary_grid_dt.gpkg')"
      ],
      "metadata": {
        "id": "PcbyIy1MCszn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soils_csv = gpd.read_file('/content/drive/MyDrive/hawaii_soils/HI soils data/combined_soc_2024_04_05.csv')"
      ],
      "metadata": {
        "id": "4T2wvedGC1aG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely.geometry import Point\n",
        "\n",
        "soils_csv = soils_csv[(soils_csv['latitude'] != '') & (soils_csv['longitude'] != '')]\n",
        "soils_csv['geometry'] = soils_csv.apply(lambda row: Point(float(row['longitude']), float(row['latitude'] )), axis=1)\n",
        "soils_gpd = gpd.GeoDataFrame(soils_csv, geometry='geometry', crs=\"EPSG:4326\")"
      ],
      "metadata": {
        "id": "VHIQgCCiD1le"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure both GeoDataFrames have the same CRS\n",
        "soils_gpd = soils_gpd.to_crs(drivers_gpd.crs)\n",
        "\n",
        "# Perform spatial join\n",
        "matched_data = gpd.sjoin_nearest(soils_gpd, drivers_gpd, how='left', distance_col='distance')"
      ],
      "metadata": {
        "id": "GUDqgLj2FDfT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_rows = matched_data[matched_data[\"depth_adj_bottom\"] == '20']\n",
        "unique_rows = unique_rows.drop_duplicates(subset=['latitude', 'longitude'])\n",
        "unique_rows = unique_rows[unique_rows['distance'] < 251]\n",
        "matched_data = unique_rows"
      ],
      "metadata": {
        "id": "DKvp_WkRIRj_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matched_data['imp_c_float'] = [float(datum) for datum in matched_data['imp_c']]"
      ],
      "metadata": {
        "id": "efng2Ra9Ho9l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "len(matched_data.select_dtypes(include=[np.number]).columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WsnTyBQKJ8r",
        "outputId": "5dad83ee-c998-4857-836c-8b54eab82551"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matched_data.columns"
      ],
      "metadata": {
        "id": "rseMUa3Crx-c",
        "outputId": "f48305f2-fe92-4bd2-87bd-b79eb071d8ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['source_dataset', 'island', 'soil_column_id', 'unique_id', 'depth_top',\n",
              "       'depth_bottom', 'depth_adj_bottom', 'latitude', 'longitude', 'c_perc',\n",
              "       'imp_c', 'bulk_density', 'imp_bd', 'hole_id', 'geometry', 'index_right',\n",
              "       'id', 'left', 'top', 'right', 'bottom', 'water', 'trees', 'grass',\n",
              "       'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built', 'bare',\n",
              "       'snow_and_ice', 'max', 'elevation', 'landform', 'SRTM_mTPI', 'aet',\n",
              "       'def', 'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx',\n",
              "       'vap', 'vpd', 'vs', 'agbd_m', 'agbd_sd', 'agbd_n', 'majorcomposition',\n",
              "       'age_years', 'age_class', 'distance', 'imp_c_float'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matched_data.columns[31]"
      ],
      "metadata": {
        "id": "HnkyaaLivP2c",
        "outputId": "0b1aad7b-cc03-480f-e2cd-15fc00e33ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'elevation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matched_data.columns[48]"
      ],
      "metadata": {
        "id": "IqeZnzowr2FK",
        "outputId": "b6f72977-712d-4d19-ce19-be410c806a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'agbd_m'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "id_fields = matched_data[['source_dataset', 'island', 'soil_column_id', 'unique_id', 'depth_top', 'depth_bottom', 'depth_adj_bottom', 'latitude', 'longitude']]\n",
        "\n",
        "keep_cols = [31,32, 33, 55]\n",
        "# Select only numeric columns\n",
        "numeric_cols = matched_data.iloc[:,keep_cols]\n",
        "numeric_cols.replace('', np.nan, inplace=True)\n",
        "numeric_cols = numeric_cols.astype(float)\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler ()\n",
        "\n",
        "# Fit the scaler on the numeric columns\n",
        "scaler.fit(numeric_cols)\n",
        "\n",
        "# Transform the numeric columns\n",
        "scaled_numeric_cols = scaler.transform(numeric_cols)\n",
        "\n",
        "# Convert the scaled numeric columns back to a DataFrame\n",
        "scaled_numeric_df = pd.DataFrame(scaled_numeric_cols, columns=numeric_cols.columns, index=numeric_cols.index)\n",
        "\n",
        "# scaled_numeric_df = scaled_numeric_df\n",
        "# Concatenate the ID fields back with the numeric columns\n",
        "numeric_df = pd.concat([id_fields, scaled_numeric_df], axis=1)"
      ],
      "metadata": {
        "id": "Lzlo7QYVFXwo",
        "outputId": "d9323c66-f85e-41d5-b888-2919907a22cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-a72e8e05ec29>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  numeric_cols.replace('', np.nan, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(scaled_numeric_df.columns)"
      ],
      "metadata": {
        "id": "eYnAyM2LsFQc",
        "outputId": "7638f0f4-2f56-4c55-df96-107c1641896a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Set the dimensions and compile the models\n",
        "latent_dim = 100\n",
        "sequence_length = len(keep_cols)  # Adjust based on your sequence length\n",
        "\n",
        "generator = build_generator(latent_dim, sequence_length)\n",
        "discriminator = build_discriminator(sequence_length)\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Set the initial learning rate\n",
        "initial_learning_rate = 0.000001\n",
        "\n",
        "# Create optimizers for the generator and discriminator\n",
        "generator_optimizer = Adam(learning_rate=initial_learning_rate, beta_1=0.5)\n",
        "discriminator_optimizer = Adam(learning_rate=initial_learning_rate, beta_1=0.5)\n",
        "gan_optimizer = Adam(learning_rate=initial_learning_rate, beta_1=0.5)\n",
        "\n",
        "# Compile the discriminator\n",
        "generator.compile(optimizer=generator_optimizer, loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Compile the discriminator\n",
        "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Compile the GAN\n",
        "gan.compile(optimizer=gan_optimizer, loss='mse')\n",
        "\n",
        "# # Assuming 'final_df' is your scaled dataset with numeric columns and ID fields\n",
        "# # Extract only the numeric columns for the GAN\n",
        "# numeric_columns = [col for col in scaled_numeric_cols.columns if scaled_numeric_cols[col].dtype in [np.float32, np.float64]]\n",
        "# real_sequences_df = scaled_numeric_cols[numeric_columns]\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "real_sequences_array = scaled_numeric_cols\n",
        "\n",
        "def real_sequence_generator(data, batch_size):\n",
        "    while True:\n",
        "        # Shuffle the data at the beginning of each epoch\n",
        "        np.random.shuffle(data)\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i + batch_size]\n",
        "            # If the batch is smaller than the batch size, pad it with samples from the beginning\n",
        "            if len(batch) < batch_size:\n",
        "                padding = data[:(batch_size - len(batch))]\n",
        "                batch = np.concatenate([batch, padding], axis=0)\n",
        "            yield batch\n",
        "\n",
        "\n",
        "# Create an instance of the generator\n",
        "# real_sequence_gen = real_sequence_generator(real_sequences_array, batch_size)\n"
      ],
      "metadata": {
        "id": "YONOBC-8BSw2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sequences, val_sequences = train_test_split(real_sequences_array, test_size=0.2, random_state=42)\n",
        "\n",
        "test_sequences, val_sequences = train_test_split(val_sequences, test_size=0.5, random_state=42)\n",
        "\n",
        "batch_size = 128  # Set the batch size\n",
        "\n",
        "\n",
        "# Define the training and validation generators\n",
        "train_gen = real_sequence_generator(train_sequences, batch_size)\n",
        "test_gen = real_sequence_generator(test_sequences, batch_size)\n",
        "val_gen = real_sequence_generator(val_sequences, batch_size)\n"
      ],
      "metadata": {
        "id": "9BDQtO5LKaGk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_indices = keep_cols[:(len(keep_cols)-1)]\n",
        "match_indices"
      ],
      "metadata": {
        "id": "ZxnviLssyxbK",
        "outputId": "d37f7746-7b17-450e-bbf7-f12605c9c396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[31, 32, 33]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the GAN with validation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_gan(generator, discriminator, gan, epochs=800, batch_size=128, latent_dim=latent_dim, val_gen=val_gen,keep_cols=keep_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpwWxtrOBXT2",
        "outputId": "accd0ada-9c4f-4c96-adaa-b9df049e8d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "Epoch: 1/800, D Loss: [0.6880215  0.53515625], G Loss: 0.23880115151405334, Val D Loss: [0.68720967 0.5546875 ]\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "8/8 [==============================] - 0s 10ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 10ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 10ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 10ms/step\n",
            "8/8 [==============================] - 0s 12ms/step\n",
            "8/8 [==============================] - 0s 13ms/step\n",
            "8/8 [==============================] - 0s 10ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "1/8 [==>...........................] - ETA: 0s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data = validate_generator()\n",
        "data = validate_generator(n_samp=1024*32)\n",
        "x_values = [ datum[0] for datum in data]\n",
        "y_values = [ datum[1] for datum in data]\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.scatter(x_values, y_values, color='blue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Difference between Real and Generated')\n",
        "plt.title('Scatter Plot of Validation Differences')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f-m4YknSb_ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar"
      ],
      "metadata": {
        "id": "Oc8rt5XeQkXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}