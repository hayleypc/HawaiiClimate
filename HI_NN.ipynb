{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hayleypc/HawaiiClimate/blob/main/HI_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Quz3PwWnrs-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a42b76-2653-4c6e-9812-483b8f3a264a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diptest\n",
            "  Downloading diptest-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from diptest) (5.9.5)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from diptest) (1.26.4)\n",
            "Downloading diptest-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.7/195.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diptest\n",
            "Successfully installed diptest-0.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "!pip install diptest\n",
        "import diptest\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from shapely.geometry import Point\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lrQb4Owry2i",
        "outputId": "22e9cc2c-35f0-41ed-b11a-bbc389716c88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK-z3uJFCryn"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "LWD2By-r9sOf"
      },
      "outputs": [],
      "source": [
        "# Preprocess data function\n",
        "def preprocess_data(matched_data,scaler = None):\n",
        "    matched_data['distance'] = 0\n",
        "    matched_data = matched_data[[str(i) == '20' for i in matched_data[\"depth_adj_bottom\"]] ]\n",
        "    matched_data['imp_c_float'] = matched_data['imp_c'].astype(float)\n",
        "\n",
        "    # Select ID fields and numeric columns\n",
        "    id_fields = matched_data[['source_dataset', 'island', 'dist_id', 'soil_column_id', 'unique_id', 'depth_top',\n",
        "                              'depth_bottom', 'depth_adj_bottom', 'latitude', 'longitude', 'x_sample', 'y_sample',\n",
        "                              'x_driver', 'y_driver']]\n",
        "    keep_cols = ['water', 'trees', 'grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built',\n",
        "                 'bare', 'snow_and_ice', 'max', 'elevation', 'landform', 'SRTM_mTPI', 'aet', 'def',\n",
        "                 'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx', 'vap', 'vpd', 'vs',\n",
        "                 'agbd_m', 'agbd_sd', 'agbd_n']\n",
        "\n",
        "    numeric_cols = matched_data[keep_cols].replace('', np.nan).astype(float).fillna(0)\n",
        "\n",
        "    # # Scale numeric columns\n",
        "    if scaler is None:\n",
        "      scaler = MinMaxScaler()\n",
        "      scaler = scaler.fit(numeric_cols)\n",
        "\n",
        "    scaled_numeric_cols = scaler.transform(numeric_cols)\n",
        "    scaled_numeric_df = pd.DataFrame(scaled_numeric_cols, columns=numeric_cols.columns, index=numeric_cols.index)\n",
        "\n",
        "    # # Scale imp_c\n",
        "    min_c = matched_data['imp_c_float'].min()\n",
        "    max_c = matched_data['imp_c_float'].max()\n",
        "    scaled_imp_c = (matched_data['imp_c_float'] - min_c) / (max_c - min_c)\n",
        "\n",
        "    # Combine ID fields with scaled numeric data\n",
        "    numeric_df = pd.concat([id_fields, scaled_numeric_df], axis=1)\n",
        "    numeric_df['imp_c_scaled'] = scaled_imp_c\n",
        "\n",
        "    return numeric_df, scaler, min_c, max_c\n",
        "    # return numeric_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "opr25fTaAKLo"
      },
      "outputs": [],
      "source": [
        "# Train model function\n",
        "def train_model(preprocessed_data):\n",
        "    keep_cols = ['water', 'trees', 'grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built',\n",
        "                 'bare', 'snow_and_ice', 'max', 'elevation', 'landform', 'SRTM_mTPI', 'aet', 'def',\n",
        "                 'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx', 'vap', 'vpd', 'vs',\n",
        "                 'agbd_m', 'agbd_sd', 'agbd_n']\n",
        "    X = preprocessed_data[keep_cols]\n",
        "    y = preprocessed_data['imp_c_scaled']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build and compile model\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_dim=X_train.shape[1]),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.3),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.3),\n",
        "        Dense(1024, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=128, verbose=1)\n",
        "\n",
        "    test_loss = model.evaluate(X_test, y_test)\n",
        "    predictions = model.predict(X_test).flatten()\n",
        "    r_squared = r2_score(y_test, predictions)\n",
        "\n",
        "    return model, test_loss, r_squared, predictions, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fMaF7fNHAMQZ"
      },
      "outputs": [],
      "source": [
        "# Predict on reserve function\n",
        "def predict_on_reserve(preprocessed_data, model, min_c, max_c):\n",
        "    keep_cols = ['water', 'trees', 'grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built',\n",
        "                'bare', 'snow_and_ice', 'max', 'elevation', 'landform', 'SRTM_mTPI', 'aet', 'def',\n",
        "                'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx', 'vap', 'vpd', 'vs',\n",
        "                'agbd_m', 'agbd_sd', 'agbd_n']\n",
        "    X = preprocessed_data[keep_cols]\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    inversed_predictions = predictions * (max_c - min_c) + min_c\n",
        "    inversed_truth = preprocessed_data['imp_c_scaled'] * (max_c - min_c) + min_c\n",
        "\n",
        "    df_out = preprocessed_data.copy()\n",
        "    df_out['predictions'] = predictions\n",
        "    df_out['inversed_predictions'] = inversed_predictions\n",
        "    df_out['inversed_imp_c'] = inversed_truth\n",
        "\n",
        "    return df_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YHFSeNNFAOm4"
      },
      "outputs": [],
      "source": [
        "# Evaluate model function\n",
        "def evaluate_model(model, X_test, y_test, inversed_predictions, inversed_truth):\n",
        "    test_loss = model.evaluate(X_test, y_test)\n",
        "    r_squared = r2_score(inversed_truth, inversed_predictions)\n",
        "\n",
        "    print(\"Test Loss:\", test_loss)\n",
        "    print(\"R-Squared Score:\", r_squared)\n",
        "\n",
        "    mae = mean_absolute_error(inversed_truth, inversed_predictions)\n",
        "    rmse = mean_squared_error(inversed_truth, inversed_predictions, squared=False)\n",
        "\n",
        "    print(\"Mean Absolute Error (MAE):\", mae)\n",
        "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "    # Scatter plot of true vs predicted values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=inversed_truth, y=inversed_predictions)\n",
        "    plt.xlabel(\"True Values\")\n",
        "    plt.ylabel(\"Predicted Values\")\n",
        "    plt.title(\"True vs Predicted Values\")\n",
        "    plt.plot([min(inversed_truth), max(inversed_truth)], [min(inversed_truth), max(inversed_truth)], 'r')\n",
        "    plt.show()\n",
        "\n",
        "    # Residual plot\n",
        "    residuals = inversed_truth - inversed_predictions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(residuals, kde=True)\n",
        "    plt.xlabel(\"Residuals\")\n",
        "    plt.title(\"Distribution of Residuals\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f-m9vVyFARKQ"
      },
      "outputs": [],
      "source": [
        "# Cross-validation and model training\n",
        "def build_model(input_shape, output_shape):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_dim=input_shape),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.3),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.3),\n",
        "        Dense(1024, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.3),\n",
        "        Dense(output_shape, activation='linear')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wfP-Yv3oAUIG"
      },
      "outputs": [],
      "source": [
        "# Define function for rescaling\n",
        "def rescale_to_minus_one_one(array):\n",
        "    return 2 * (array - array.min()) / (array.max() - array.min()) - 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PO2AwaR9EgHi"
      },
      "outputs": [],
      "source": [
        "def train_model_a(preprocess_data):\n",
        "\n",
        "    keep_cols = ['water', 'trees','grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built',\n",
        "                'bare', 'snow_and_ice', 'max', 'elevation', 'landform', 'SRTM_mTPI','aet', 'def',\n",
        "                'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn','tmmx', 'vap', 'vpd', 'vs',\n",
        "                'agbd_m', 'agbd_sd', 'agbd_n']\n",
        "\n",
        "    X = preprocess_data[keep_cols]\n",
        "\n",
        "    y = preprocess_data['imp_c_scaled']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define model\n",
        "    def build_model(input_shape):\n",
        "        model = Sequential([\n",
        "            Dense(256, activation='relu', input_dim=input_shape),\n",
        "            BatchNormalization(),\n",
        "            LeakyReLU(alpha=0.2),\n",
        "            Dropout(0.3),\n",
        "            Dense(512, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            LeakyReLU(alpha=0.2),\n",
        "            Dropout(0.3),\n",
        "            Dense(1024, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            LeakyReLU(alpha=0.2),\n",
        "            Dropout(0.3),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    model = build_model(X_train.shape[1])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=128, verbose=0)\n",
        "\n",
        "    test_loss = model.evaluate(X_test, y_test)\n",
        "    predictions = model.predict(X_test).flatten()\n",
        "    r_squared = r2_score(y_test, predictions)\n",
        "\n",
        "    return model, test_loss, r_squared, predictions, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iL2LBgDCywp"
      },
      "source": [
        "# Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3Kevt1Sgrz7V"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "gdf = gpd.read_file('/content/drive/MyDrive/hawaii_soils/HI soils data/2024_update/annotated_combo_imputed_SOC.gpkg')\n",
        "drivers_gpd = gpd.read_file('/content/drive/MyDrive/hawaii_soils/Analysis Data/250_summary_grid_dt.gpkg')\n",
        "soils_csv = pd.read_csv('/content/drive/MyDrive/hawaii_soils/HI soils data/combined_soc_2024_04_05.csv')\n",
        "labeled_dist = pd.read_csv('/content/drive/MyDrive/hawaii_soils/labeled_distr_annote.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8ECmlVaxvys7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8ce237-8fa5-43f9-ee8f-ac6f127343c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/geopandas/array.py:365: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Preprocess soils data\n",
        "soils_csv = soils_csv.dropna(subset=['latitude', 'longitude'])\n",
        "soils_csv['geometry'] = soils_csv.apply(lambda row: Point(float(row['longitude']), float(row['latitude'])), axis=1)\n",
        "soils_gpd = gpd.GeoDataFrame(soils_csv, geometry='geometry', crs=\"EPSG:4326\")\n",
        "\n",
        "# Merge and transform GeoDataFrames\n",
        "soils_gpd = pd.merge(soils_gpd, gdf[['dist_id', 'unique_id']], on='unique_id', how='inner')\n",
        "soils_gpd = soils_gpd.to_crs(drivers_gpd.crs)\n",
        "\n",
        "# Perform spatial join\n",
        "matched_data = gpd.sjoin_nearest(soils_gpd, drivers_gpd, how='left', distance_col='distance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "M0me_uTZv4cn"
      },
      "outputs": [],
      "source": [
        "# Additional preprocessing steps\n",
        "drivers_gpd['x_driver'] = drivers_gpd.geometry.x\n",
        "drivers_gpd['y_driver'] = drivers_gpd.geometry.y\n",
        "soils_gpd['x_sample'] = soils_gpd.geometry.x\n",
        "soils_gpd['y_sample'] = soils_gpd.geometry.y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soils_gpd = pd.merge(soils_gpd, labeled_dist,  on='dist_id')"
      ],
      "metadata": {
        "id": "gPSv3sFl3pj6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aVUvkzHBv7dz"
      },
      "outputs": [],
      "source": [
        "# Reproject both GeoDataFrames to the same CRS\n",
        "soils_buffered = soils_gpd.to_crs(epsg=32604)\n",
        "drivers_gpd = drivers_gpd.to_crs(soils_buffered.crs)\n",
        "\n",
        "# Apply buffer in the same CRS\n",
        "soils_buffered['x_sample'] = soils_buffered.geometry.x\n",
        "soils_buffered['y_sample'] = soils_buffered.geometry.y\n",
        "soils_buffered.geometry = soils_buffered.geometry.buffer(1000)\n",
        "\n",
        "# Perform spatial join\n",
        "matches_within_distance = gpd.sjoin(soils_buffered, drivers_gpd, how='left', predicate='intersects')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matches_within_distance = pd.merge(labeled_dist, matches_within_distance, on='dist_id')"
      ],
      "metadata": {
        "id": "s2VedzbMxHu2"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_cols = ['water', 'trees', 'grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built',\n",
        "              'bare', 'snow_and_ice', 'max', 'elevation', 'landform', 'SRTM_mTPI', 'aet', 'def',\n",
        "              'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx', 'vap', 'vpd', 'vs',\n",
        "              'agbd_m', 'agbd_sd', 'agbd_n']\n",
        "\n",
        "numeric_cols = drivers_gpd[keep_cols].replace('', np.nan).astype(float).fillna(0)"
      ],
      "metadata": {
        "id": "ZQe7tDl_eYHd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_scaler = MinMaxScaler()\n",
        "global_scaler = global_scaler.fit(numeric_cols)"
      ],
      "metadata": {
        "id": "bTl8RpJoeJgZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4932WNga2cJ6",
        "outputId": "e1bd40bd-4276-46bd-ee1c-04c755aceaf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-515e653087b1>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  matched_data['imp_c_float'] = matched_data['imp_c'].astype(float)\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data and get scaled values\n",
        "numeric_df, scaler, min_c, max_c = preprocess_data(matches_within_distance,scaler=global_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_scaler.transform(drivers_gpd[keep_cols])"
      ],
      "metadata": {
        "id": "tcftM5kQe8Sk",
        "outputId": "cb0e7d64-aa38-4f94-996c-c74a3a99fe74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01971653, 0.12002313, 0.70418756, ..., 0.00233966, 0.00482294,\n",
              "        0.25806452],\n",
              "       [0.02109351, 0.31880874, 0.51522931, ..., 0.00957597, 0.02872613,\n",
              "        0.22580645],\n",
              "       [0.02058384, 0.37253208, 0.41890304, ..., 0.0052422 , 0.01274947,\n",
              "        0.27419355],\n",
              "       ...,\n",
              "       [0.01593273, 0.5023984 , 0.12296759, ..., 0.0135864 , 0.01395604,\n",
              "        0.22580645],\n",
              "       [0.01965143, 0.48245845, 0.1508686 , ..., 0.01201557, 0.02169049,\n",
              "        0.25806452],\n",
              "       [0.02497371, 0.34661654, 0.20406703, ..., 0.01391951, 0.02132026,\n",
              "        0.40322581]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming numeric_df needs to have dist_id included, merge or join it if necessary\n",
        "# numeric_df = numeric_df.merge(matches_within_distance[['dist_id']], on='some_common_column', how='left')\n",
        "\n",
        "dict_list = []\n",
        "\n",
        "for dist_id in matches_within_distance['dist_id'].unique():\n",
        "    dist_df = pd.DataFrame({\n",
        "        \"dist_id\": [dist_id],\n",
        "        \"labeled_dist\": soils_gpd[(soils_gpd['dist_id'] == dist_id) & (soils_gpd['depth_adj_bottom'] == 20)]['labeled_dist_h'].unique(),\n",
        "        \"imp_c\": [soils_gpd[(soils_gpd['dist_id'] == dist_id) & (soils_gpd['depth_adj_bottom'] == 20)]['imp_c'].tolist()],\n",
        "        \"imp_c_quantile\": [np.quantile(soils_gpd[(soils_gpd['dist_id'] == dist_id) & (soils_gpd['depth_adj_bottom'] == 20)]['imp_c'], np.linspace(0, 1, 10)).tolist()]\n",
        "    })\n",
        "    dict_list.append(dist_df)\n",
        "\n",
        "# Concatenate all DataFrames in dict_list to form a final DataFrame if needed\n",
        "dist_df = pd.concat(dict_list, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "xRrZ3Ff4dzvE"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxfmjFNQC3iJ"
      },
      "source": [
        "# Prepare random forest classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS3RfOz_DPsP"
      },
      "source": [
        "## Random forest specific functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Bimodal and multimodal generators\n",
        "\n",
        "def generate_bimodal_distribution(size, mean1, std1, mean2, std2, proportion1):\n",
        "    \"\"\"\n",
        "    Generate a random bimodal distribution.\n",
        "\n",
        "    Parameters:\n",
        "        size (int): Total number of samples to generate.\n",
        "        mean1 (float): Mean of the first distribution.\n",
        "        std1 (float): Standard deviation of the first distribution.\n",
        "        mean2 (float): Mean of the second distribution.\n",
        "        std2 (float): Standard deviation of the second distribution.\n",
        "        proportion1 (float): Proportion of samples from the first distribution.\n",
        "\n",
        "    Returns:\n",
        "        np.array: An array of samples from the bimodal distribution.\n",
        "    \"\"\"\n",
        "    size1 = int(size * proportion1)\n",
        "    size2 = size - size1\n",
        "    samples1 = np.random.normal(mean1, abs(std1), size1)\n",
        "    samples2 = np.random.normal(mean2, abs(std2), size2)\n",
        "    return np.concatenate([samples1, samples2])\n",
        "\n",
        "def multimodal_generator(n, mean, sd, mean_variation, sd_variation):\n",
        "    # bimodal_data = generate_bimodal_distribution(size=1000, mean1=-1*mean, std1=np.random.normal(0, sd_variation, 1), mean2=mean, std2=np.random.normal(0, sd_variation, 1), proportion1=np.random.uniform(.4, .6, 1)[0])\n",
        "    # bi_dist =  np.quantile(bimodal_data, np.linspace(0, 1, n))\n",
        "    variable_mean = mean + np.random.normal(0, mean_variation)\n",
        "    variable_sd = sd + abs(np.random.normal(0, sd_variation))\n",
        "    dist = np.random.normal(variable_mean, variable_sd, int(n*1.5))\n",
        "    dist.sort()\n",
        "    dist_a = dist[-int(n):]\n",
        "\n",
        "    variable_mean = mean+(1+mean*1.5+sd*1.5) + np.random.normal(0, mean_variation)\n",
        "    variable_sd = sd + abs(np.random.normal(0, sd_variation))\n",
        "    dist = np.random.normal(variable_mean, variable_sd, int(n*1.5)) +(1+mean_variation*1.5+sd*1.5)\n",
        "    dist.sort()\n",
        "    dist_b = dist[:int(n)]\n",
        "\n",
        "    bi_dist = np.concatenate([dist_a,dist_b])\n",
        "    return bi_dist"
      ],
      "metadata": {
        "id": "Ip6w_FynuOJ2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "EKQhCxxUDUFh"
      },
      "outputs": [],
      "source": [
        "def normal_generator(n, mean, sd, mean_variation, sd_variation):\n",
        "    variable_mean = mean + np.random.normal(0, mean_variation)\n",
        "    variable_sd = sd + abs(np.random.normal(0, sd_variation))\n",
        "    return np.random.normal(variable_mean, variable_sd, n)\n",
        "\n",
        "def uniform_generator(n, min_val, max_val, min_variation, max_variation):\n",
        "    variable_min = min_val + np.random.uniform(-min_variation, min_variation)\n",
        "    variable_max = max_val + np.random.uniform(-max_variation, max_variation)\n",
        "    return np.random.uniform(variable_min, variable_max, n)\n",
        "\n",
        "def left_tailed_generator(n, mean, sd, mean_variation, sd_variation):\n",
        "    variable_mean = mean + np.random.normal(0, mean_variation)\n",
        "    variable_sd = sd + abs(np.random.normal(0, sd_variation))\n",
        "    dist = np.random.normal(variable_mean, variable_sd, int(n*1.5))\n",
        "    dist.sort()\n",
        "    return dist[:int(n)]\n",
        "\n",
        "def right_tailed_generator(n, mean, sd, mean_variation, sd_variation):\n",
        "    variable_mean = mean + np.random.normal(0, mean_variation)\n",
        "    variable_sd = sd + abs(np.random.normal(0, sd_variation))\n",
        "    dist = np.random.normal(variable_mean, variable_sd, int(n*1.5))\n",
        "    dist.sort()\n",
        "    return dist[-int(n):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "00J-3sF6DVB_"
      },
      "outputs": [],
      "source": [
        "def rescale_to_minus_one_one(values):\n",
        "    min_value = np.min(values)\n",
        "    max_value = np.max(values)\n",
        "    scaled_values = 2 * ((values - min_value) / (max_value - min_value)) - 1\n",
        "    return scaled_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "fZDSdtSwDXDf"
      },
      "outputs": [],
      "source": [
        "def generate_samples(n, q, generator_func, *args):\n",
        "    samples = generator_func(n, *args)\n",
        "    samples_q = np.quantile(samples, np.linspace(0, 1, q))\n",
        "    return rescale_to_minus_one_one(samples_q)\n",
        "\n",
        "def generate_normal_samples(n=10000, q=10, mean_value=0, sd_value=1, mean_variation=0.5, sd_variation=0.5):\n",
        "    return generate_samples(n, q, normal_generator, mean_value, sd_value, mean_variation, sd_variation)\n",
        "\n",
        "def generate_uniform_samples(n=10000, q=10, min_value=-1, max_value=1, min_variation=0.5, max_variation=0.5):\n",
        "    return generate_samples(n, q, uniform_generator, min_value, max_value, min_variation, max_variation)\n",
        "\n",
        "def generate_right_tailed_samples(n=10000, q=10, mean=0, sd=1, mean_variation=0.5, sd_variation=0.5):\n",
        "    return generate_samples(n, q, right_tailed_generator, mean, sd, mean_variation, sd_variation)\n",
        "\n",
        "def generate_left_tailed_samples(n=10000, q=10, mean=0, sd=1, mean_variation=0.5, sd_variation=0.5):\n",
        "    return generate_samples(n, q, left_tailed_generator, mean, sd, mean_variation, sd_variation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_multimodal_samples(n=10000, q=10, mean=0, sd=1, mean_variation=0.5, sd_variation=0.5):\n",
        "#     # samples = generate_samples(n, q, left_tailed_generator, mean, sd, mean_variation, sd_variation)\n",
        "#     return generate_samples(n, q, multimodal_generator, mean, sd, mean_variation, sd_variation)"
      ],
      "metadata": {
        "id": "RU4tmEM3MLyh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ogNva38SDbyZ"
      },
      "outputs": [],
      "source": [
        "def generate_training_data(n_samples=10000):\n",
        "    data_list = {}\n",
        "\n",
        "    # Generate samples for each distribution type\n",
        "    data_list['normal'] = [generate_normal_samples() for _ in range(n_samples)]\n",
        "   # data_list['bimodal'] = [generate_multimodal_samples() for _ in range(n_samples)]\n",
        "    data_list['uniform'] = [generate_uniform_samples() for _ in range(n_samples)]\n",
        "    data_list['right_tailed'] = [generate_right_tailed_samples() for _ in range(n_samples)]\n",
        "    data_list['left_tailed'] = [generate_left_tailed_samples() for _ in range(n_samples)]\n",
        "\n",
        "    # Combine all data into a single DataFrame\n",
        "    combined_data = pd.DataFrame()\n",
        "    for name, samples_list in data_list.items():\n",
        "        df = pd.DataFrame(samples_list)\n",
        "        df['label'] = name\n",
        "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "    # Assign column names\n",
        "    combined_data.columns = [f'V{i+1}' for i in range(combined_data.shape[1] - 1)] + ['label']\n",
        "\n",
        "    return combined_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0phs9ADczH"
      },
      "source": [
        "## Train random forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def left_tailed_generator(n, mean, sd, mean_variation, sd_variation):\n",
        "    variable_mean = mean + np.random.normal(0, mean_variation)\n",
        "    variable_sd = sd + abs(np.random.normal(0, sd_variation))\n",
        "    dist = np.random.normal(variable_mean, variable_sd, int(n*1.5))\n",
        "    dist.sort()\n",
        "    return dist[-int(n):]"
      ],
      "metadata": {
        "id": "21jmfUoENrs3"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # data = np.quantile(multimodal_generator(100, 1,  1,1,1),np.linspace(0, 1, 10))\n",
        "# # data = multimodal_generator(100, 1,  1,1,1)\n",
        "\n",
        "# # Create a histogram plot\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.hist(data, bins=30, color='blue', alpha=0.7)\n",
        "# plt.title('Histogram of Random Data')\n",
        "# plt.xlabel('Values')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ijLmNpl9N6My"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Quantile plot\n",
        "# plt.figure(figsize=(8, 4))\n",
        "# percentiles = [10,20,30,40,50,60,70,80,90,100]\n",
        "# plt.plot(percentiles, np.quantile(multimodal_generator(100, 1,  1,1,1),np.linspace(0, 1, 10)), marker='o')\n",
        "# plt.title('Quantile Plot')\n",
        "# plt.xlabel('Percentiles')\n",
        "# plt.ylabel('Quantile Values')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "0rkpKlj-67UD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist_df['imp_c_quantile_scaled'] = [rescale_to_minus_one_one(dist) for dist in dist_df[\"imp_c_quantile\"]]"
      ],
      "metadata": {
        "id": "nCod5-Zs7pQj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dist_df.columns"
      ],
      "metadata": {
        "id": "L4k-6e748Cyl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_true = label_encoder.inverse_transform(y_encoded).tolist()"
      ],
      "metadata": {
        "id": "Teq2rShTBQwL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training_data = generate_training_data()"
      ],
      "metadata": {
        "id": "bE2_qeVRCFdC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CpxXSdi8DqIv",
        "outputId": "7b087246-ce81-43e3-eedf-0e147aba4d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-671431f2c83f>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# Print the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \"\"\"\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    441\u001b[0m             )\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Save for later; cross validation random forest for truth data\n",
        "\n",
        "# Separate features and labels\n",
        "X = pd.DataFrame(dist_df['imp_c_quantile_scaled'].tolist(), columns=[f'col{i}' for i in range(1, 11)])\n",
        "y = dist_df['labeled_dist']\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_pred_list = []\n",
        "for i in range(len(X)):\n",
        "\n",
        "\n",
        "  # Initialize the Random Forest Classifier\n",
        "  rf_model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=50, min_samples_leaf=2)\n",
        "\n",
        "  # Train the model\n",
        "  rf_model.fit(np.array(X.drop(index= i)), np.concatenate([y[:i], y[i+1:]]))\n",
        "\n",
        "  # Print the trained model\n",
        "  print(rf_model)\n",
        "\n",
        "  # Predict the responses for test dataset\n",
        "  y_pred =  rf_model.predict(X)[i]\n",
        "  y_pred_list.append(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=50, min_samples_leaf=2)\n",
        "\n",
        "# Train the model\n",
        "# rf_model.fit(np.array(training_data.drop('label', axis=1)),training_data['label'])\n",
        "rf_model.fit(np.array(X),y)"
      ],
      "metadata": {
        "id": "mV2gwiwLuD_0",
        "outputId": "519c9124-7f4c-41a6-b79b-eba5a9230b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_leaf_nodes=50, min_samples_leaf=2, n_estimators=500)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_list = rf_model.predict(X)\n",
        "y_true = y"
      ],
      "metadata": {
        "id": "_HQVP4xMHC_w"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred_list))\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred_list))\n",
        "\n",
        "# Precision, Recall, F1-Score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_list))\n",
        "\n",
        "# If it's a binary classification, calculate ROC-AUC\n",
        "if len(np.unique(y_pred_list)) == 2:\n",
        "    y_probs = rf_model.predict_proba(X_test)[:, 1]  # probability estimates for the positive class\n",
        "    fpr, tpr, thresholds = roc_curve(y_pred_list, y_probs)\n",
        "    print(\"AUC:\", auc(fpr, tpr))\n"
      ],
      "metadata": {
        "id": "cr2DM2Njugee",
        "outputId": "bdb465b2-9da2-4230-a861-04a9548b6703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [77, 11]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-1f84aab398a8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \"\"\"\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [77, 11]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLt0CBTkC9VZ"
      },
      "source": [
        "# Cross validation loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation: loop through all dist_ids, one at a time\n",
        "# subset data to exclude unlabeled distributions 1st\n",
        "\n",
        "data = numeric_df.dropna(subset=['dist_id'])\n",
        "\n",
        "unique_dist_ids = data['dist_id'].unique()\n",
        "np.random.shuffle(unique_dist_ids)\n"
      ],
      "metadata": {
        "id": "zME6ms1_8EtI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set_lists = [unique_dist_ids[i:i + 5] for i in range(0, len(unique_dist_ids), 5)]"
      ],
      "metadata": {
        "id": "jfSWblBj8FuE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # set_lists\n",
        "set_lists =[np.array([73,  1, 60, 18, 58]),\n",
        "            np.array([23, 76, 51, 63, 66]),\n",
        "            np.array([31, 47, 30, 20,  5]),\n",
        "            np.array([15, 77, 43, 53, 12]),\n",
        "            np.array([ 3, 56, 29, 75, 13]),\n",
        "            np.array([70, 42, 65, 52, 54]),\n",
        "            np.array([46, 41, 50, 40, 82]),\n",
        "            np.array([67, 24, 84, 61,  6]),\n",
        "            np.array([26, 74, 17,  2, 44]),\n",
        "            np.array([81, 71, 49, 45, 88]),\n",
        "            np.array([22, 11, 86, 83, 14]),\n",
        "            np.array([33, 27, 25, 59, 38]),\n",
        "            np.array([68, 39, 78, 69, 87]),\n",
        "            np.array([16, 19, 48, 21, 79]),\n",
        "            np.array([57, 72, 80, 64, 28]),\n",
        "            np.array([62, 85])]"
      ],
      "metadata": {
        "id": "JQqmcwLV9EAH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_lists[5:]"
      ],
      "metadata": {
        "id": "fEwFk7EiauLW",
        "outputId": "958acccc-5c3a-4024-e8c7-30e7a880d3bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([70, 42, 65, 52, 54]),\n",
              " array([46, 41, 50, 40, 82]),\n",
              " array([67, 24, 84, 61,  6]),\n",
              " array([26, 74, 17,  2, 44]),\n",
              " array([81, 71, 49, 45, 88]),\n",
              " array([22, 11, 86, 83, 14]),\n",
              " array([33, 27, 25, 59, 38]),\n",
              " array([68, 39, 78, 69, 87]),\n",
              " array([16, 19, 48, 21, 79]),\n",
              " array([57, 72, 80, 64, 28]),\n",
              " array([62, 85])]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in range(len(set_lists))][14:]"
      ],
      "metadata": {
        "id": "zeI9PAC0T31P",
        "outputId": "644fb56b-7188-4d43-e129-1727f1f5cc3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14, 15]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "jI3ZzmvJhezb"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "VpM7JLhTwKJ8",
        "outputId": "fb175564-435a-40d7-8ab4-43048bdf60d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'set_lists' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-b9bbd5aecd2a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mset_list_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0munique_dist_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset_list_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'set_lists' is not defined"
          ]
        }
      ],
      "source": [
        "for set_list_id in [i for i in range(len(set_lists))][14:]:\n",
        "  unique_dist_ids = set_lists[set_list_id]\n",
        "  result_list = []\n",
        "  import gc\n",
        "  gc.collect()\n",
        "  # Loop over each dist_id, leaving one out at time\n",
        "  for xval_id in unique_dist_ids:\n",
        "      r2_list = []\n",
        "      loss_list = []\n",
        "      prediction_list = []\n",
        "      matched_data_list = []\n",
        "      model_list = []\n",
        "      # Reserve data for validation\n",
        "      xval_data = data[data['dist_id'] == xval_id]\n",
        "\n",
        "      # Matched data for training\n",
        "      train_data = data[data['dist_id'] != xval_id]\n",
        "\n",
        "      reserve_data = train_data.groupby('unique_id').sample(n=1)\n",
        "\n",
        "      for i in range(10):\n",
        "          matched_data = train_data.groupby('unique_id').sample(n=1)\n",
        "          matched_data = matched_data.reset_index(drop=True)\n",
        "\n",
        "          model, test_loss, r_squared, predictions, y_test = train_model_a(matched_data)\n",
        "\n",
        "          predictions = predict_on_reserve(reserve_data, model, min_c, max_c)\n",
        "\n",
        "          matched_data_list.append(matched_data)\n",
        "          prediction_list.append(predictions)\n",
        "          model_list.append(model)\n",
        "          loss_list.append(test_loss)\n",
        "          r2_list.append(r_squared)\n",
        "\n",
        "      combined_array = np.array([predictions['predictions'].values for predictions in prediction_list])\n",
        "      combined_array[combined_array < 0 ] = 0\n",
        "\n",
        "      arr_min = np.min(combined_array.flatten())\n",
        "      arr_max = np.max(combined_array.flatten())\n",
        "\n",
        "      dist_array = [np.sort(np.array([i[j] for  i in combined_array])) for j in range(combined_array.shape[1])]\n",
        "      norm_dist_array =  [np.sort((np.array([i[j] for  i in combined_array]) - arr_min) / (arr_max-arr_min)) for j in range(combined_array.shape[1])]\n",
        "\n",
        "      keep_cols = ['water', 'trees','grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built',\n",
        "                  'bare', 'snow_and_ice', 'max', 'elevation', 'landform', 'SRTM_mTPI','aet', 'def',\n",
        "                  'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn','tmmx', 'vap', 'vpd', 'vs',\n",
        "                  'agbd_m', 'agbd_sd', 'agbd_n']\n",
        "\n",
        "      scaled_numeric_df = prediction_list[0][keep_cols]\n",
        "\n",
        "      scaled_numeric_df['norm_dist_array'] = norm_dist_array\n",
        "      scaled_numeric_df.dropna(inplace=True)\n",
        "\n",
        "      scaled_numeric_df['norm_dist_array'] = scaled_numeric_df['norm_dist_array'].to_list()\n",
        "\n",
        "      norm_dist_array = np.array([i for i in scaled_numeric_df['norm_dist_array']])\n",
        "\n",
        "      X = scaled_numeric_df[keep_cols]\n",
        "      y = norm_dist_array\n",
        "\n",
        "      # X = scaled_numeric_df.iloc[:, :-1]\n",
        "      # y = scaled_numeric_df.iloc[:, -1]\n",
        "\n",
        "      # x = np.array(X)\n",
        "      # y = np.array(norm_dist_array)\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "      model = build_model(X_train.shape[1],output_shape=10)\n",
        "\n",
        "      model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error',metrics = ['mae'])\n",
        "      model.fit(X_train, y_train, validation_split=0.2, epochs=600, batch_size=128, verbose=0)\n",
        "      model.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error',metrics = ['mae'])\n",
        "      model.fit(X_train, y_train, validation_split=0.2, epochs=600, batch_size=128, verbose=0)\n",
        "\n",
        "      test_loss = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "      predictions = model.predict(xval_data[keep_cols])\n",
        "      imp_c_scaled = np.sort(xval_data.groupby('unique_id').sample(n=1)['imp_c_scaled']*max_c+min_c)\n",
        "\n",
        "      imp_c_quantiles = np.quantile(imp_c_scaled, np.linspace(0, 1,10))\n",
        "\n",
        "      scaled_predictions = predictions*max_c+min_c\n",
        "      prediction_quantiles = [np.quantile(prediction, np.linspace(0, 1,10)) for prediction in scaled_predictions]\n",
        "\n",
        "      prediction_rf_ready_predictions = [rescale_to_minus_one_one(sorted(prediction)) for prediction in  predictions]\n",
        "\n",
        "      # Use the trained model to make predictions\n",
        "      predicted_labels_encoded = rf_model.predict(prediction_rf_ready_predictions)\n",
        "\n",
        "      # Decode the encoded labels back to original labels\n",
        "      predicted_predicted_labels = predicted_labels_encoded #here\n",
        "\n",
        "      real_rf_ready = [rescale_to_minus_one_one(sorted(prediction)+ np.random.uniform(-0.001, 0.001, prediction.shape[0])) for prediction in  [imp_c_quantiles]]\n",
        "\n",
        "      # Use the trained model to make predictions\n",
        "      real_labels_encoded = rf_model.predict(real_rf_ready)\n",
        "\n",
        "\n",
        "      # Decode the encoded labels back to original labels\n",
        "      real_predicted_labels = real_labels_encoded #here\n",
        "\n",
        "      lat_lon = [Point(xy) for xy in zip(xval_data['latitude'],xval_data['longitude'])]\n",
        "      xy_sample = [Point(xy) for xy in zip(xval_data['x_sample'],xval_data['y_sample'])]\n",
        "      xy_driver = [Point(xy) for xy in zip(xval_data['x_driver'],xval_data['y_driver'])]\n",
        "\n",
        "      result_dict ={\n",
        "                    \"xval_id\": xval_id,\n",
        "                    \"lat_lon\": lat_lon,\n",
        "                    \"xy_sample\": xy_sample,\n",
        "                    \"xy_driver\": xy_driver,\n",
        "                    \"imp_c_scaled\": imp_c_scaled,\n",
        "                    \"imp_c_quantiles\": imp_c_quantiles,\n",
        "                    \"real_predicted_labels\": real_predicted_labels,\n",
        "                    \"scaled_predictions\": scaled_predictions,\n",
        "                    \"prediction_quantiles\": prediction_quantiles,\n",
        "                    \"predicted_predicted_labels\": predicted_predicted_labels\n",
        "                  }\n",
        "      result_list.append(result_dict)\n",
        "\n",
        "  import pickle\n",
        "  # Specify the filename\n",
        "  filename = f'/content/drive/MyDrive/hawaii_soils/result_list_{set_list_id}.pkl'\n",
        "\n",
        "  # Open the file in binary write mode and save the result_list\n",
        "  with open(filename, 'wb') as file:\n",
        "      pickle.dump(result_list, file)\n",
        "\n",
        "  print(f\"Data has been pickled and saved to {filename}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result_list)"
      ],
      "metadata": {
        "id": "UQzDIPdhzz0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_list = [pickle.load(open(f'/content/drive/MyDrive/hawaii_soils/result_list_{set_list_id}.pkl', 'rb')) for set_list_id in range(len(set_lists))]"
      ],
      "metadata": {
        "id": "8kpinP9rZtXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xval_id = 0"
      ],
      "metadata": {
        "id": "RVc191GtGEDf"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_list = []\n",
        "loss_list = []\n",
        "prediction_list = []\n",
        "matched_data_list = []\n",
        "model_list = []\n",
        "# Reserve data for validation\n",
        "# xval_data = data[data['dist_id'] == xval_id]\n",
        "\n",
        "# Matched data for training\n",
        "train_data = data\n",
        "\n",
        "reserve_data = train_data.groupby('unique_id').sample(n=1)\n",
        "\n",
        "for i in range(10):\n",
        "    matched_data = train_data.groupby('unique_id').sample(n=1)\n",
        "    matched_data = matched_data.reset_index(drop=True)\n",
        "\n",
        "    model, test_loss, r_squared, predictions, y_test = train_model_a(matched_data)\n",
        "\n",
        "    predictions = predict_on_reserve(reserve_data, model, min_c, max_c)\n",
        "\n",
        "    matched_data_list.append(matched_data)\n",
        "    prediction_list.append(predictions)\n",
        "    model_list.append(model)\n",
        "    loss_list.append(test_loss)\n",
        "    r2_list.append(r_squared)\n",
        "\n",
        "combined_array = np.array([predictions['predictions'].values for predictions in prediction_list])\n",
        "combined_array[combined_array < 0 ] = 0\n",
        "\n",
        "arr_min = np.min(combined_array.flatten())\n",
        "arr_max = np.max(combined_array.flatten())\n",
        "\n",
        "dist_array = [np.sort(np.array([i[j] for  i in combined_array])) for j in range(combined_array.shape[1])]\n",
        "norm_dist_array =  [np.sort((np.array([i[j] for  i in combined_array]) - arr_min) / (arr_max-arr_min)) for j in range(combined_array.shape[1])]\n",
        "\n",
        "keep_cols = ['water', 'trees','grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built',\n",
        "            'bare', 'snow_and_ice', 'max', 'elevation', 'landform', 'SRTM_mTPI','aet', 'def',\n",
        "            'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn','tmmx', 'vap', 'vpd', 'vs',\n",
        "            'agbd_m', 'agbd_sd', 'agbd_n']\n",
        "\n",
        "scaled_numeric_df = prediction_list[0][keep_cols]\n",
        "\n",
        "scaled_numeric_df['norm_dist_array'] = norm_dist_array\n",
        "scaled_numeric_df.dropna(inplace=True)\n",
        "\n",
        "scaled_numeric_df['norm_dist_array'] = scaled_numeric_df['norm_dist_array'].to_list()\n",
        "\n",
        "norm_dist_array = np.array([i for i in scaled_numeric_df['norm_dist_array']])\n",
        "\n",
        "X = scaled_numeric_df[keep_cols]\n",
        "y = norm_dist_array\n",
        "\n",
        "# X = scaled_numeric_df.iloc[:, :-1]\n",
        "# y = scaled_numeric_df.iloc[:, -1]\n",
        "\n",
        "# x = np.array(X)\n",
        "# y = np.array(norm_dist_array)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = build_model(X_train.shape[1],output_shape=10)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error',metrics = ['mae'])\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=600, batch_size=128, verbose=0)\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error',metrics = ['mae'])\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=600, batch_size=128, verbose=0)\n",
        "\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "# predictions = model.predict(xval_data[keep_cols])\n",
        "# imp_c_scaled = np.sort(xval_data.groupby('unique_id').sample(n=1)['imp_c_scaled']*max_c+min_c)\n",
        "\n",
        "# imp_c_quantiles = np.quantile(imp_c_scaled, np.linspace(0, 1,10))\n",
        "\n",
        "# scaled_predictions = predictions*max_c+min_c\n",
        "# prediction_quantiles = [np.quantile(prediction, np.linspace(0, 1,10)) for prediction in scaled_predictions]\n",
        "\n",
        "# prediction_rf_ready_predictions = [rescale_to_minus_one_one(sorted(prediction)) for prediction in  predictions]\n",
        "\n",
        "# # Use the trained model to make predictions\n",
        "# predicted_labels_encoded = rf_model.predict(prediction_rf_ready_predictions)\n",
        "\n",
        "# # Decode the encoded labels back to original labels\n",
        "# predicted_predicted_labels = predicted_labels_encoded #here\n",
        "\n",
        "# real_rf_ready = [rescale_to_minus_one_one(sorted(prediction)+ np.random.uniform(-0.001, 0.001, prediction.shape[0])) for prediction in  [imp_c_quantiles]]\n",
        "\n",
        "# # Use the trained model to make predictions\n",
        "# real_labels_encoded = rf_model.predict(real_rf_ready)\n",
        "\n",
        "\n",
        "# # Decode the encoded labels back to original labels\n",
        "# real_predicted_labels = real_labels_encoded #here\n",
        "\n",
        "# lat_lon = [Point(xy) for xy in zip(xval_data['latitude'],xval_data['longitude'])]\n",
        "# xy_sample = [Point(xy) for xy in zip(xval_data['x_sample'],xval_data['y_sample'])]\n",
        "# xy_driver = [Point(xy) for xy in zip(xval_data['x_driver'],xval_data['y_driver'])]"
      ],
      "metadata": {
        "id": "e3q1vDK9Fo4c",
        "outputId": "6c884134-70b4-4a3f-b945-91e4fd7670fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0207\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0178\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0196\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0181\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0181\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0186\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0188\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0165\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0171\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0157\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-92-82c6ef15b89b>:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  scaled_numeric_df['norm_dist_array'] = norm_dist_array\n",
            "<ipython-input-92-82c6ef15b89b>:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  scaled_numeric_df.dropna(inplace=True)\n",
            "<ipython-input-92-82c6ef15b89b>:47: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  scaled_numeric_df['norm_dist_array'] = scaled_numeric_df['norm_dist_array'].to_list()\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0015 - mae: 0.0280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_gpd = global_scaler.transform(drivers_gpd[keep_cols])"
      ],
      "metadata": {
        "id": "pt9zjJTqFoA7"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_drivers = model.predict(numeric_gpd)"
      ],
      "metadata": {
        "id": "XYtuSJ7BIfn8",
        "outputId": "83c2bdf5-467d-4007-e07b-cec116cf835c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11224/11224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pred_drivers"
      ],
      "metadata": {
        "id": "ASVqgJpFJkDw"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numeric_gpd[80]"
      ],
      "metadata": {
        "id": "gyhIiqcWMDAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_predictions = predictions*max_c+min_c\n",
        "prediction_quantiles = [np.quantile(prediction+ np.random.uniform(-0.001, 0.001, prediction.shape[0]), np.linspace(0, 1,10)) for prediction in predictions]\n",
        "\n",
        "prediction_rf_ready_predictions = [rescale_to_minus_one_one(sorted(prediction)+ np.random.uniform(-0.001, 0.001, prediction.shape[0])) for prediction in  predictions]\n",
        "\n",
        "nan_index = [np.isnan(array).any() for array in prediction_rf_ready_predictions]\n",
        "\n",
        "prediction_rf_ready_predictions = [np.nan_to_num(array, nan=0.0) for array in prediction_rf_ready_predictions]\n",
        "# # Use the trained model to make predictions\n",
        "predicted_labels_encoded = rf_model.predict(prediction_rf_ready_predictions)"
      ],
      "metadata": {
        "id": "R874-R2AI3XF"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gXM_BkbPNe_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_gpd['distribution_type'] = predicted_labels_encoded"
      ],
      "metadata": {
        "id": "Szeafc2ZNTeh"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_gpd['nan_index'] = nan_index"
      ],
      "metadata": {
        "id": "hUcqD54qNJsr"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantile_df = pd.DataFrame(scaled_predictions)\n",
        "quantile_df.columns = [f'Quantile_{i+1}' for i in range(quantile_df.shape[1])]\n"
      ],
      "metadata": {
        "id": "yk4SLMufOABa"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_gpd.join(quantile_df).to_file(\"predicted_results.gpkg\")"
      ],
      "metadata": {
        "id": "v3qo5g9cNmOt"
      },
      "execution_count": 100,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO6yT/VNRJSsecQDn6RJ9wn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}